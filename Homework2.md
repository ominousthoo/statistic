# Homework 2
## Introduction
In the class of the second lecture about the Regression in econometrics model, the lecturer has introduced the variance-bias tradeoff between the low-degree polynomial and high-degree polynomial model. The variance-bias tradeoff is explaining that if the regression model is more simple, they will be tend to have a high level of bias and lower level of variance. Thus, if we increase the complexity of the model by increasing the degree of variables in the polynomial model, the bias of the estimator might reduce but the level of variance will be going to increase sharply.<br/>
The Data Analysis will be run in `R Studio` and the data using will be generated randomly using `R`.<br/>
The study will shows the difference of bias and variance between different degree of polynomial model.<br/>

## Data Description
In this study, the sample data (xi,yi) will be generated randomly through `R Studio`. Variable `xi` will be generated randomly by following uniform distribution between minimum value of `1` and maximum value of `100`. The variable `yi` will be generated as normal distribution with the mean `log(x)` and standard deviation = `1.2`. <br/>
The code and the fucntion below shows the process of generating data.<br/>
``` r
#define the function of generating data
f <- function(x){
  log(x)
}
#generating data at sample size of 150, x values between range 1 to 100
#y values follow normal distribution with mean = log(x) and standard deviation = 1.2 
gen_data_func <- function(f,size=150) {
  x <- runif(size,min=1,max=100)
  y <- rnorm(size,f(x),1.2)
  data.frame(x,y)
}
#use set.seed for reproducibility of the data
set.seed(112233)
data1=gen_data_func(f)
#view and check the reliability of data
View(data1)
```
After generating the data we have to use later in the data analysis, let us check the data we just generated by illustrating them with a ScatterPlot Diagram and fitted line. In the ScatterPlot Diagram below, a linear fitted line and lowess fitted line have been used to compare the difference. The ScatterPlot has been generated by using the codes below in `R Studio`.
``` r
#plot both the data point x and y in the scatter plot
plot(y~x, data=data1, main="Scatter Plot", 
     xlab="x",ylab="y",pch=19)
#create a legend to show the linear fitted line and lowess line
legend("bottomright", 
       c("Linear","Lowess"), 
       col = c("red","blue"), lwd = 2,cex=0.75)
#draw the linear fitted line and lowess line in the graph
abline(lm(y~x,data=data1),col="red")
lines(lowess(data1$x,data1$y),col="blue")
```
The data point of `x` and `y` has been shown using scatterplot below.<br/>
**Illustration of Scatterplot**<br/>
![ScatterPlot](https://github.com/ominousthoo/statistic/blob/Data-files/Rplot02.png)<br/>
From the ScatterPlot Diagram above we can see that the generated data point are scattering around but the linear fitted line and lowess line are still roughly showing us the shape of the line which `y=log(x)` should have even it is not really obvious.<br/>
## Data Analysis and Simulation
After generating the data set which we have to use in the simulation, now we have to generate a few polynomial regression model with different degree in order to compare the differences of the bias and variance of their estimator accordingly. In this study, I will be going to simulate by using four different degree of polynomial model which is degree 1, degree 2, degree 5 and degree 10. The process of generating polynomial model is using the codes below.
``` r
#4 polynomial regression model has been generated on different degree
reg1=lm(y~poly(x,degree = 1), data=data1)
reg2=lm(y~poly(x,degree = 2), data=data1)
reg3=lm(y~poly(x,degree = 5), data=data1)
reg4=lm(y~poly(x,degree = 10), data=data1)
```
After generating the polynomial regression model, all polynomial model has been ran and the regression line has been plotted onto the ScatterPlot Diagram in order to compare the differences between the regression line of different degree. The true line which is `y=log(x)` will be plotted as black solid line for comparison purpose in the graph. Referring to the figure below, it clearly shows that the regression line of the polynomial model with degree 10 which is the yellow line is very close to each data points but in fact, it is having the most variation or in other words, furthest away from the true line `y=log(x)`.<br/>
The Graph of Regression Line is generated by using the codes below by using `R Studio`.
``` r
#use set.seed for reproducibility of the data
set.seed(123)
#create the scatter plot as the base of the graph for regression lines
plot(y ~ x, data = data1, col = "grey", pch = 20,
     main = "Regression Line Between Different Degree")
grid()
#create a suitable scale for the x-axis
grid = seq(from =0 , to = 100, by = 1)
lines(grid, f(grid), col = "black", lwd = 1)#real regression line
#regression line for different degree of polynomial model
lines(grid, predict(reg1, newdata = data.frame(x = grid)), col = "green",  lwd = 2,lty = 2)
lines(grid, predict(reg2, newdata = data.frame(x = grid)), col = "636",   lwd = 2, lty =3)
lines(grid, predict(reg3, newdata = data.frame(x = grid)), col = "654", lwd = 2, lty = 4)
lines(grid, predict(reg4, newdata = data.frame(x = grid)), col = "367",  lwd = 2, lty = 5)
#create a legend to show clearer about the regression line
legend("bottomright", 
       c("degree of 1","degree of 2","degree of 5","degree of 10", "real"), 
       col = c("green", "636", "654", "367", "black"), lty = c(2, 3, 4, 5, 1), lwd = 2,cex=0.75)
```
![Regression Line](https://github.com/ominousthoo/statistic/blob/Data-files/Rplot03.png)<br/>
After illustrating the regression line above, the next step is to create a matrix of estimation for the different model in order to calculate and compare the bias, variance and mean square error of the estimator between the polynomial model with different degree. In this simulation, the value of `x = 20` has been picked in order to measure and compare the performance between different polynomial model. For the real situation, `log(20)` will be equal to `2.996` and this will be used to measure the performance of the polynomial model by measuring how far was the estimation of the model is from the real result which is `2.996`. Thus, the codes will be shown below.
``` r
set.seed(112233) #use set.seed for reproducibility of the data
nrun <- 200 #calculate for 200 estimation in each model
nreg <- 4 #total 4 model has been build
x <- data.frame(x=20) #this will be the real data to be measured
#log(20) = 2.996
estm = matrix(0,nrow=nrun,ncol=nreg) # create a matrix with 4 columns
for (sam in 1:nrun){                 #and 200 rows to store the generated data
  data1 = gen_data_func(f)
  reg1=lm(y~poly(x,degree = 1), data=data1)
  reg2=lm(y~poly(x,degree = 2), data=data1)
  reg3=lm(y~poly(x,degree = 5), data=data1)
  reg4=lm(y~poly(x,degree = 10), data=data1)
  
  estm[sam,1]=predict(reg1,x) #fill the matrix with the estimation
  estm[sam,2]=predict(reg2,x)
  estm[sam,3]=predict(reg3,x)
  estm[sam,4]=predict(reg4,x)
}
```
After we generated the estimation matrix of each polynomial model, now we have to generate the formula or the equation in order to calculate the Mean Squared Error, Variance and the Bias of the estimator for each polynomial regression in order to compare the changes of bias and variance when the polynomial degree increase in the model. The `R` codes will be attached as follow.
``` r
#generate a function to calculate Mean Square Error
mse = function(real,estimate){
  mean((estimate-real)^2)
}
#generate a function to calculate Bias of estimator
bias = function(estimate,real){
  mean(estimate)-real
}
#generate a function to calculate the Variance of estimator
var = function(estimate){
  mean((estimate-mean(estimate))^2)
}
#by using apply function, calculate the bias, variance and MSE
#of each polynomial regression at x=20
bias1 = apply(estm, 2, bias,real=f(x=20))
var1 = apply(estm,2,var)
mse1 = apply(estm,2,mse,real=f(x=20))
```
After generating the function to calculate Variance, Bias and MSE, the `R` codes below will be showing us the result of the simulation. The result below clearly shows the Variance-Bias tradeoff between different degree of polynomial regression which is when the degree of polynomial increase, the Bias of estimator will decrease and the Variance will increase.
``` r
results = data.frame(
+   poly_degree = c(1, 2, 5, 10),
+   round(mse1, 4),
+   round(bias1 ^ 2, 4),
+   round(var1, 4)
+ )
> colnames(results) = c("Degree", "MSE", "Squared Bias", "Variance")
> rownames(results) = NULL
> knitr::kable(results, booktabs = TRUE, escape = TRUE, align = "c")
```
## Result
The result of Variance-Bias Tradeoff has been shown below<br/>
Thus, the simulation has clearly shows that the bias of estimator will decrease alongs the increase of the polynomial degree but simultaneously, the variance will be increased too.
```
| Degree |  MSE   | Squared Bias | Variance |
|:------:|:------:|:------------:|:--------:|
|   1    | 0.0638 |    0.0357    |  0.0281  |
|   2    | 0.0802 |    0.0560    |  0.0241  |
|   5    | 0.0567 |    0.0026    |  0.0541  |
|   10   | 0.1011 |    0.0000    |  0.1010  |
```
<br/>
<br/>
### Reference
*David Dalpiaz - R for Statistical Learning*<br/>
*GitHub Help - Basic writing and formatting syntax*<br/>
*DataCamp - Quick R-Tutorial*<br/>
*GitHub Guide - Mastering Markdown*<br/>
*Scott Fortmann-Roe - Understanding The Bias-Variance Tradeoff*<br/>
<br/>
*You can access to my GitHub pages for Homework 2 [here](https://github.com/ominousthoo/statistic/blob/Homework/Homework2.md).*
